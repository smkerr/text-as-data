---
title: "Assignment 4"
author: "Steve Kerr (211924)"
date: "20 November 2023"
output: pdf
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this assignment, you are asked to use topic modelling to investigate manifestos from the manifesto project maintained by [WZB](https://manifesto-project.wzb.eu/). You can either use the UK manifestos we looked at together in class, or collect your own set of manifestos by choosing the country/countries, year/years and party/parties you are interested in. You should produce a report which includes your code, that addresses the following aspects of creating a topic model, making sure to answer the questions below.

This time, you will be assessed not only on whether the code gets the right result, but on how you understand and communicate your understanding of the modelling process and how this can answer your research question. The best research question is one that is interesting and answerable, but the most important thing is that the research question is answerable with the methods you choose.

You will also be assessed on the presentation of your results, and on the concision and readability of your code.

## 1. Data acquisition, description, and preparation

Bring together a dataset from the WZB. What years, countries and parties are included in the dataset? How many texts do you have for each of these?

Prepare your data for topic modelling by creating a document feature matrix. Describe the choices you make here, and comment on how these might affect your final result.

### Setup

> We start by loading our packages.

```{r}
# TODO: discard irrelevant packages
# load packages
pacman::p_load(
  dplyr, 
  ggplot2,
  gt, 
  here,
  kableExtra,
  knitr,
  lubridate,
  manifestoR,
  quanteda,
  readr,
  tidyr
  #tidytext,
  #topicmodels,
  #LDAvis
  #stm
  )
```

### Data acquisition

> We'll be using WZB's `manifestoR` package to download and work with the manifesto data. In order to establish a connection with WZB's API, we first need to set our API key.

```{r}
# define API key
mp_setapikey(here("Assignments_SK/Assignment-4/manifesto_apikey.txt"))
```

> We limit our analysis to **the United Kingdom** and **the United States**. I examine the period **from XXXX to 2020** (all years for which data are available). Furthermore, I exclude parties with less than XXX over the XXX-year period. In total, the analysis includes **XXX parties**: the United Kingdom (X) and the United States (X). In total, our corpus consists of **XX party manifestos**.
>
> We start by checking for the availability of manifestos.

```{r}
# list of countries
countries <- c(
  #"United Kingdom", 
  "United States"
  )

# list of parties
parties <- c(
  #"Conservative Party", "Labour Party", "Liberal Party", # UK parties
  #"Liberal Democrats", "Democratic Unionist Party", "Scottish National Party",
  #"Socialist Party", "Conservative People’s Party", 
  "Democratic Party", "Republican Party" # US parties
  )

# min date
#min_date <- 200401

available_docs <- mp_availability(countryname %in% countries &
                                    partyname %in% parties)

annotated_docs <- available_docs |> 
  filter(annotations == TRUE)
```

> Next, we load our corpus with the manifestos we've identified as relevant.

```{r, message=FALSE}


min_year <- 200401

# define file path for corpus
corpus_path <- here("Assignments_SK/Assignment-4/data/corpus.rds")

mp_availability(countryname %in% countries & 
                  partyname %in% parties & 
                  date >= min_year
                )

# download data if doesn't already exist
if (!file.exists(corpus_path)) {

  corpus <- mp_corpus(annotated_docs) # query data
  saveRDS(corpus, corpus_path) # save data

} else corpus <- read_rds(corpus_path) # load data

# TODO: Make sure that no duplicates included
# TODO: Make sure that all texts are in English

# inspect corpus
corpus
head(content(corpus[[1]])) # view beginning of text of first manifesto
NLP::meta(corpus[[1]]) # view meta data of first manifesto
```

> Next, we load the main dataset which contains meta data for each manifesto as well as labels for which passages correspond with which topics.

```{r}
# define file path for main dataset
mp_path <- here("Assignments_SK/Assignment-4/data/mp_data.rds")

# download data if doesn't already exist
if (!file.exists(mp_path)) {
  
  mp_df <- mp_maindataset() # query data
  saveRDS(mp_df, mp_path) # save data
  
} else mp_df <- read_rds(mp_path) # load data

# clean main dataset
mp_df <- mp_df |> 
  filter(countryname %in% countries & partyname %in% parties)

# TODO: Make sure that no duplicates are included

head(mp_df)
```

### Data description

> Here's a breakdown of countries and parties contained in our dataset.

```{r}
# TODO: make table look nice
# table of countries and parties
mp_df |> 
  count(countryname, partyname) |> 
  arrange(countryname, desc(n)) |> 
  select(-n) |> 
  gt(
    rowname_col = "partyname",
    groupname_col = "countryname",
    row_group_as_column = TRUE
  )
```

```{r, fig.height=5}
# TODO: fix plot
# plot number of manifestos by country over time
mp_df |> 
  mutate(year = year(edate)) |> 
  count(year, countryname) |>
  ggplot(aes(x = year, y = n, fill = countryname)) +
  geom_col(width = 1) +
  scale_fill_brewer(palette = "Paired") + 
  labs(
    title = "Party Manifestos",
    subtitle = "2000-2020",
    x = NULL,
    y = "No. Manifestos",
    fill = NULL,
    caption = "source: WZB"
  ) +
  facet_wrap(~ countryname, ncol = 1) +
  scale_x_continuous(breaks = seq(1920, 2020, 5)) +
  scale_y_continuous(breaks = seq(0, 6, 1)) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
    )
```

> We can observe differences in election cycles between the United States and the United Kingdom. Wheras the US has a predictable election cycle with elections scheduled to take place once every four years, the United Kingdom's parliamentarian systems lends itself to a much more erratic election schedule (?). Note that the number of manifestos issued by major parties (as defined per our inclusion criteria) remains the same across time for the United States (2), while this figure fluctuates in the United Kingdom (?).

### Data preparation

> Next, we prepare our data for topic modeling by transforming our corpus into a document feature matrix. As part of pre-processing, we .... Note that we also.... This may impact our final result by ....

```{r}
# interested in free market economy 
mp_describe_code(code = "401")
```

```{r}
meta_df <- mp_metadata(countryname %in% countries &
                         partyname %in% parties &
                         date > min_year) # TODO: rename var to min_date

glimpse(meta_df)

meta_df 

meta_df |> 
  filter(annotations == TRUE) |> 
  pull(manifesto_id)
```

```{r}
glimpse(mp_df)

mp_df |>
  pull(per401)

test <- corpus |> 
  as.data.frame(with.meta = TRUE)
```

```{r}
# initialize empty df to store text data
df <- data.frame("manifesto_id" = character(), "manifesto" = character())

# create df from corpus
for (i in 1:length(corpus)) {
    
  temp_df <- tibble(manifesto_id = NLP::meta(corpus[[i]])$manifesto_id, 
                   manifesto = content(corpus[[i]]) |> paste(collapse = " "))
  df <- bind_rows(df, temp_df)
  
}

# only include pertinent manifest IDs
df <- filter(df, manifesto_id %in% manifesto_ids)

head(df)
```

> Next, we perform pre-processing on our text data.

```{r}
#library(quanteda)

# pre-processing
dfmat <- df$manifesto |> 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) |> # TODO: remove more?
  tokens_remove(pattern = stopwords("en")) |> 
  tokens_wordstem() |> 
  dfm() |>  # TODO: dfm_trim more?
  dfm_trim(min_termfreq = 5) # remove infrequent terms 
  #dfm_tfidf()
  
rownames(dfmat) <- df$manifesto_id # add IDs to row names

head(dfmat)
```

## 2. Research question

Describe a research question you want to explore with topic modelling. Comment on how answerable this is with the methods and data at your disposal.

> For my research question, I am interested in exploring the extent to which climate-related issues were included in various parties' manifestos. Specifically, I am interested in whether a party's willingness to express a stance on climate change is more related to their political leaning or the country.

## 3. Topic model development

Create a topic model using your data. Explain to a non-specialist what the topic model does. Comment on the choices you make here in terms of hyperparameter selection and model choice. How might these affect your results and the ability to answer your research question?

```{r}
library(topicmodels)
#library(lda)
#library(LDAvis)

# define number of topics
n_topics <- 5

tictoc::tic() # timer start
lda <- LDA(dfmat, n_topics) # TODO: experiment with number of topics
tictoc::toc() # timer stop
```

```{r}
library(tidytext)
print(dim(lda@gamma))
print(dim(lda@beta))

topic_words <- tidy(lda, matrix = "beta") |>
  group_by(topic) |>
  slice_max(beta, n = 5) |>
  ungroup() |>
  arrange(topic, -beta)
topic_words
```

```{r}
topic_words |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
ggsave(glue::glue("img/top_terms_{n_topics}.png"), width = 12, height = 8)
```

```{r}
# manifesto_topics <- tidy(lda, matrix = "gamma") |>
#   left_join(merged_df, by = c("document" = "manifesto_id"))
# 
# yearly_topics <- manifesto_topics |> 
#   #filter(country == "United States") |> 
#   mutate(year = year(date)) |> 
#   group_by(year, party, topic) |>
#   summarise(gamma = sum(gamma)) |>
#   group_by(year, party) |>
#   mutate(year_share = gamma/sum(gamma)) |>
#   ungroup() |>
#   mutate(topic = factor(topic))
# 
# yearly_topics
# 
# yearly_topics |> 
#   filter(topic %in% 1:5) |> 
#   ggplot(aes(x = year, y = year_share, group = topic, colour = topic, fill = topic)) +
#   geom_line() + 
#   facet_wrap(vars(party), ncol = 1) +
#   theme_minimal()
# 
# # save plot
# ggsave(glue::glue("img/topic_groups_{n_topics}.png"), width = 12, height = 8)
# 
# library(LDAvis)
# topicmodels2LDAvis <- function(x, ...){
#  post <- topicmodels::posterior(x)
#  if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
#  mat <- x@wordassignments
#  json <- LDAvis::createJSON(
#    phi = post[["terms"]],
#    theta = post[["topics"]],
#    vocab = colnames(post[["terms"]]),
#    doc.length = slam::row_sums(mat, na.rm = TRUE),
#    term.frequency = slam::col_sums(mat, na.rm = TRUE)
#  )
#  return(json)
# }
# json <- topicmodels2LDAvis(lda)
# serVis(json)
```

```{r}
#The definition of a “good” topic is not universal but task dependent.

#A good topic model is one that helps us to answer the research question we have in mind, or helps us to better perform the task we have.

#Sometimes we want the big picture (few topics), sometimes we want fine-grained detail (many topics).

#What we should ensure is that any results we present are not an artefact of an arbitrary model choice we make, but are robust to a variety of reasonable specifications.
```

```{r}
# evaluate performance
#A quick heuristic for naming topics is to concatenate the top 3 terms for each topic.
#If we want to use our model then we should give meaningful names to topics by inspecting the top terms and top documents associated with each topic.

#Loss/heldout-likelihood based measures work by comparing the topic predictions of words in documents to the actually observed numbers of words in documents.

#Coherence based measures work by assessing whether the words in a topic are similar. If words in a topic only infrequently co-occur in documents, then this topic is considered less coherent.
```

## 4. Topic model description

Describe the topic model. What topics does it contain? How are these distributed across the data?

```{r}

```

## 5. Answering your research question

Use your topic model to answer your research question by showing plots or statistical results. Discuss the implications of what you find, and any limitations inherent in your approach. Discuss how the work could be improved upon in future research.

```{r}

```

# Sources

-   Lehmann, Pola / Franzmann, Simon / Burst, Tobias / Regel, Sven / Riethmüller, Felicia / Volkens, Andrea / Weßels, Bernhard / Zehnter, Lisa (2023): The Manifesto Data Collection. Manifesto Project (MRG/CMP/MARPOR). Version 2023a. Berlin: Wissenschaftszentrum Berlin für Sozialforschung (WZB) / Göttingen: Institut für Demokratieforschung (IfDem). <https://doi.org/10.25522/manifesto.mpds.2023a>

# Resources

-   [`manifestoR` vignette](https://cran.r-project.org/web/packages/manifestoR/vignettes/manifestoRworkflow.pdf)
