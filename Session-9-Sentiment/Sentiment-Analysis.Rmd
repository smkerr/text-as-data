---
title: "Sentiment Analysis"
author: "Max Callaghan"
date: "2023-11-13"
output: 
  beamer_presentation:
    latex_engine: xelatex
    keep_tex: true
    citation_package: natbib
    theme: "Singapore"
    highlight: zenburn
    includes:
      in_header: columns.tex
extra_dependencies: "mathtools"
classoption: aspectratio=169  
fontsize: 10pt
urlcolor: blue
bibliography: ../presentation-resources/MyLibrary.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(reticulate)
library(quanteda)
library(lexicon)
use_python("/usr/bin/python3.10")
library(dplyr)
library(ggplot2)
```



```{python include=FALSE}
import math
import pandas as pd
```

# Introduction and Objectives

## Assignment 4

Assignment 4 is still live. If you have issues, or encounter difficulties, raise an issue on the Github repository, or write me an email!

## Assignment 5

Assignment 5 is approaching, and you should have a clear idea of what you want to do by the end of next week.

Feel free to ask for quick feedback on any ideas you have in the coming days

## Objectives

By now we have spent a long time understanding how to **represent** texts in simple and more complex ways.

We've also started asking questions about texts: What are they about?

Today we will ask a new question about texts: what sentiment does it express?


# Intro to sentiment analysis

## What is a sentiment?

The emotion embodied in a text. Often reduced to positive-negative, but can encompass a more complex range of emotions like joy, sadness, anger. 

## Sentiment analysis as classification

In some ways, sentiment analysis is a special case of the broader **classification** task, where we ask:

- Is a text of class A or not? or,
- Is a text of class A or B

With sentiment analysis, the classes are inherent features of how humans use language which *generalise* (to a certain extent) across contexts. 

## An overview of techniques to do sentiment analysis

Doing sentiment analysis usually involves **rule-based** or **statistical machine-learning** techniques

\begin{itemize}
  \item<2->Assessing sentiment based on counting words have a predefined sentiment
  \item<3->Using a classifier that has been trained to identify sentiment with text examples that have been labelled.
\end{itemize}

# Lexicon-based sentiment analysis

## Positive and negative words

We know about the "bag of words" model of representing texts.

We also know that some words are rather positive, whereas some are rather negative.

Consider the texts:

\medskip

```{r echo=TRUE, include=TRUE, message=FALSE}
texts <- c(
  "Elon Musk is a champion of free speech",
  "It's a terrible shame to see mashed potato thrown at art"
)
```

\medskip

Do they express positive or negative sentiment? How can we tell?


## Using Lexicons in R

We can import a lexicon in R using tidytext. Each row, contains a word and its value

\medskip

```{r echo=TRUE, include=TRUE, message=FALSE}
library(tidytext)
library(dplyr)
lex <- get_sentiments("afinn")
sample_n(lex, 5)
```

## Using Lexicons in R

Note that the Afinn lexicon is not the newest version. We can just read this in directly from the author's Github page.

\medskip

```{r echo=TRUE, include=TRUE, message=FALSE, cache=TRUE}
library(readr)
lex <- read_tsv(
  "https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt",
  col_names=c("word","value")
)

lex

```

## Using Lexicons in R

There are a few different lexicons, compiled by different authors, using different techniques involving amazon turk and author knowledge, which encode different types of emotions.

\medskip

```{r echo=TRUE, include=TRUE, message=FALSE}
library(tidytext)
library(dplyr)
lex <- get_sentiments("nrc")
head(lex)
```
## Using Lexicons in R

We can also put our usual document feature matrix into a similar format

\medskip
\scriptsize

```{r echo=TRUE, include=TRUE, message=FALSE}
library(quanteda)
dfmat <- texts %>%
  tokens %>%
  dfm()

text_tokens <- tidy(dfmat)
head(text_tokens)
```

## Tidy lexicons

Now we can join these to see which words in the texts have what sentiment

\medskip
\scriptsize

```{r echo=TRUE, include=TRUE, message=FALSE, cache=TRUE}
lex <- read_tsv("https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt", col_names=c("word","value"))
dfmat <- texts %>%
  tokens %>%
  dfm()

text_tokens <- tidy(dfmat) %>% 
  inner_join(lex, by=c("term" = "word"))

text_tokens
```

## Tidy lexicons

We can then just sum word scores for each document to get a sentiment score for that document

\medskip
\scriptsize

```{r echo=TRUE, include=TRUE, message=FALSE}

doc_sentiments <- tidy(dfmat) %>% 
  inner_join(lex, by=c("term" = "word")) %>%
  mutate(value=value*count) %>%
  group_by(document) %>%
  summarise(value = sum(value))

doc_sentiments
```
## Using Lexicons in Python

Doing this in Python is very similar. We can use the afinn package to access the afinn lexion

\medskip
\scriptsize

```{python echo=TRUE, include=TRUE, message=FALSE}
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from afinn import Afinn
import numpy as np

afinn = Afinn()
lex = pd.DataFrame(afinn._dict.items(),columns=["word","value"])

lex.head()
```

## Using Lexicons in Python

Then, in the same way, we can put our dfm into a "tidy" form and merge with our lexicon

\medskip
\scriptsize

```{python echo=TRUE, include=TRUE, message=FALSE}
texts = [
    "Elon Musk is a champion of free speech",
    "It's a terrible shame to see mashed potato thrown at art"
]
vec = CountVectorizer()
dfmat = vec.fit_transform(texts)

def tidy_dfmat(dfmat, vec):
    nz = dfmat.nonzero()
    text_df = pd.DataFrame({
        "document": np.array(texts)[nz[0]],
        "term": vec.get_feature_names_out()[nz[1]],
        "count": dfmat[nz].A1
    })
    return text_df
text_df = tidy_dfmat(dfmat, vec)
text_tokens = text_df.merge(lex, left_on="term", right_on="word")
text_tokens["value"] *= text_tokens["count"]
text_tokens
```

## Using Lexicons in Python

In the very same way, we can sum across documents
\medskip
\scriptsize

```{python echo=TRUE, include=TRUE, message=FALSE}
doc_sentiments = text_tokens.groupby("document")["value"].sum()
doc_sentiments
```


\medskip

Although we can also get the sentiment from a text directly

```{python echo=TRUE, include=TRUE, message=FALSE}
scores = [afinn.score(t) for t in texts]
for text, score in zip(texts,scores):
  print(text, score)
```


## VADER

VADER represents just about the state of the art in lexicon-based sentiment analysis, and is especially suitable for social media texts.

It also incorporates rules that extend it beyond the bag-of-words model

## 5 Heuristics 

The Vader [paper](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109/8122) identifies 5 heuristics that extend just counting words from a lexicon, and implements these in their algorithm.

\begin{itemize}
  \item<1->Punctuation (!) increases the magnitude of the sentiment: "Food here is good!!" > "Food here is good"
  \item<2->CAPITALIZATION increaeses the magnitude of the sentiment: "Food here is GREAT" > "Food here is great"
  \item<3->Degree modifiers impact intensity > or <. "Service is marginally good" < "service is good" < "service is extremely good".
  \item<4->"But" signals shift in sentiment, and that second clause is stronger: "Food here is good, but the service is bad" -> Overall more negative than positive
  \item<5->Negations in a tri-gram preceeding a sentiment-laden feature flip the polarity
\end{itemize}

## Exercise: trying VADER out

Try out a few examples of texts with VADER. See if you can show how the rules it employs work, and see if you can "trick" the algorithm.

\medskip

```{r echo=TRUE, include=TRUE, message=FALSE}
library(vader)
get_vader("This text is a test")
```

\medskip

```{python echo=TRUE, include=TRUE, message=FALSE}
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
analyzer.polarity_scores("This text is a test")
```

## VADER in practice

Let's load a [dataset](https://doi.org/10.7910/DVN/RQ7P1F) of tweets from the VoteYes campaign from the Scottish independence referendum. We can calculate sentiment for each tweet using `vader_df()`.

Let's look at the most positive tweets

\medskip
\scriptsize

```{r echo=TRUE, warning=FALSE, include=TRUE, message=FALSE, cache=TRUE}
library(vader)
yes <-read_csv("../datasets/YesScotlandTweets_cleaned.csv")
yes$campaign <- "YesScotland"
no <- read_csv("../datasets/UkTogetherTweets_cleaned.csv")
no$campaign <- "UkTogether"
tweets <- rbind(yes, no)

sentiments <- vader_df(tweets$text)
tweet_sentiment <- cbind(tweets, select(sentiments,-text))

pos <- tweet_sentiment %>% arrange(desc(compound)) %>% 
  head()

for( i in rownames(pos) ) {
  print(pos[i, "text"])
  print(pos[i, "compound"])
}
```
## VADER in practice

Let's load a [dataset](https://doi.org/10.7910/DVN/RQ7P1F) of tweets from the VoteYes and UkTogether campaigns from the Scottish independence referendum. We can calculate sentiment for each tweet using `vader_df()`.

Let's look at the most negative tweets

\medskip
\scriptsize

```{r echo=TRUE, warning=FALSE, include=TRUE, message=FALSE, cache=TRUE}
neg <- tweet_sentiment %>% arrange(compound) %>% 
  head()

for( i in rownames(neg) ) {
  print(neg[i, "text"])
  print(neg[i, "compound"])
}
```
## Sentiment over time

We can also look at how sentiment changed over time by taking the mean compound score in each time period, for each campaign group. Given the regular week-weekend variation, it also makes sense to show the 7 day rolling mean.

\medskip
\scriptsize

```{r echo=TRUE, warning=FALSE, include=TRUE, message=FALSE, cache=TRUE}
library(tidyr)
tweet_sentiment$date <- as.Date(tweet_sentiment$created) 
wide_sentiment <- tweet_sentiment %>% 
  group_by(campaign, date) %>%
  summarise(score = mean(compound)) %>%
  pivot_wider(names_from=campaign, values_from=score)

days <- data.frame(date=seq(as.Date("2014-06-01"),as.Date("2014-09-18"),1))

daily_sentiment <- days %>% left_join(wide_sentiment) %>%
  pivot_longer(cols=-date, names_to="campaign", values_to="score") %>%
  group_by(campaign) %>% arrange(date) %>%
  mutate(score7 = data.table::frollmean(score, 7))

daily_sentiment %>% head()
```

## Sentiment over time

Now that we have the data in the format we want, we can plug this into ggplot2

\medskip

:::::: {.cols data-latex=""}

::: {.col data-latex="{0.5\textwidth}"}
\scriptsize

```{r echo=TRUE, warning=FALSE, include=TRUE, message=FALSE, cache=TRUE, fig.show='hide'}
library(ggplot2)
ggplot(daily_sentiment, aes(date, colour=campaign)) + 
  geom_point(aes(y=score),size=0.5) + 
  geom_line(aes(y=score7))

ggsave("plots/sentiment_time.png", width=6, height=3)
```
:::

::: {.col data-latex="{0.05\textwidth}"}
\ 
:::

::: {.col data-latex="{0.45\textwidth}"}

\begin{figure}
\includegraphics[width=\linewidth]{plots/sentiment_time.png}
\end{figure}

:::

::::::

## VADER with Python

In python, we can use the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) package

\medskip
\scriptsize

```{python echo=TRUE, include=TRUE, message=FALSE}
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

yes = pd.read_csv("../datasets/YesScotlandTweets_cleaned.csv")
yes["campaign"] = "YesScotland"
no = pd.read_csv("../datasets/UkTogetherTweets_cleaned.csv")
no["campaign"] = "UkTogether"
df = pd.concat([yes, no]).reset_index(drop=True)

analyzer = SentimentIntensityAnalyzer()

results = [analyzer.polarity_scores(x) for x in df["text"]]
sentiment = pd.DataFrame.from_dict(results)

sentiment_df = pd.concat([df,sentiment], axis=1)
sentiment_df.head()
```

## VADER with Python

We get the data into the format where there is a row for every day, and the score for each day is in a column for each campaign

\medskip
\scriptsize

```{python echo=TRUE, include=TRUE, message=FALSE}
import matplotlib.pyplot as plt
sentiment_df["date"] = pd.to_datetime(df.created).dt.date
daily_sentiment = (sentiment_df
                   .groupby(["date","campaign"])["compound"]
                   .mean()
                   .reset_index()
                   .pivot_table(columns="campaign", values="compound", index="date")
                   .reset_index()
                  )

daily_sentiment["date"] = pd.to_datetime(daily_sentiment["date"])
days = pd.date_range(start="2014-06-01",end="2014-09-18")
daily_sentiment = pd.DataFrame({"date": days}).merge(daily_sentiment, how="left")

daily_sentiment.head()
```

## Sentiment over time

The easiest way to plot this is with base matplotlib

\medskip

:::::: {.cols data-latex=""}

::: {.col data-latex="{0.5\textwidth}"}
\scriptsize

```{python echo=TRUE, warning=FALSE, include=TRUE, message=FALSE, cache=TRUE, fig.show='hide'}
fig, ax = plt.subplots(figsize=(6,4))

for campaign in ["UkTogether", "YesScotland"]:
    ax.scatter(
      daily_sentiment.date, daily_sentiment[campaign],
      s=5, alpha=0.7
    )
    x = daily_sentiment[campaign].rolling(7).mean()
    ax.plot(daily_sentiment.date, x, label=campaign)

fig.autofmt_xdate()
ax.legend()

plt.savefig("plots/sentiment_time_py.png")
```
:::

::: {.col data-latex="{0.05\textwidth}"}
\ 
:::

::: {.col data-latex="{0.45\textwidth}"}

\begin{figure}
\includegraphics[width=\linewidth]{plots/sentiment_time_py.png}
\end{figure}

:::

::::::

## Sentiment analysis in other languages

- [GerVADER](https://github.com/KarstenAMF/GerVADER)
- [Spanish Emotion Lexicon](https://www.cic.ipn.mx/~sidorov/#SEL)

# Fancy sentiment analysis

## Fancy sentiment analysis

Fancy NLP does not apply rules that we give it. It *learns* rules from training data. 

Complex models, which encode text in complex ways, have outperformed lexicon-based sentiment analysis *on the main benchmarked tasks for which they are often optimized*.

Sentiment datasets are often comprised of movie or product reviews.

## Fancy sentiment analysis

We will learn more about how training such models work in the next sessions, but you can access one of many such models [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment). See also [Spanish](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis), [German](https://huggingface.co/oliverguhr/german-sentiment-bert), [Brazilian Portuguese](https://huggingface.co/turing-usp/FinBertPTBR), [Chinese](https://huggingface.co/uer/roberta-base-finetuned-chinanews-chinese)

# Validation

## Validation

Almost all methods for sentiment analysis are validated, but almost none are validated on your dataset. Unless your dataset is very similar to the validation dataset, you should validate yourself.


This means selecting a random sample of your texts, labelling the sentiment of these texts by hand, then comparing the label you gave with the score given by your method.

If your method gives the same label as you in 100\% of cases, then you have an accuracy of 100%.

We will explore validation metrics in the next session on supervised learning.

## Exercise - validating on your own data

- In a spreadsheet, write 5 texts that express your feelings about recent political events, with each text in a new row of the same column
- Ask your neighbour to rate the sentiment of your texts in the neighbouring column. The value should be between -1 (completely negative) and 1 (completely positive).
- Apply one of the techniques we have learnt today to your texts and compare with the human annotation
- What is the average (absolute) difference between your human label and the automated technique?
- For what text is the divergence greatest?

# Examples in research

## Tones from a Narrowing Race

In [Tones from a Narrowing Race: Polling and Online Political Communication during the 2014 Scottish Referendum Campaign](https://www.cambridge.org/core/journals/british-journal-of-political-science/article/tones-from-a-narrowing-race-polling-and-online-political-communication-during-the-2014-scottish-referendum-campaign/49CEE09374F4729B4C0B7048FBA4521C), Evelyne Brie and Yannick Dufresne set out to investigate the dynamics of how political campaigns use negative communication.

They use the data that we've just seen: tweets sent by the pro-independence and pro-union campaigns in the Scottish independence referendum of 2014.

## Tones from a Narrowing Race: Obectives

The authors want to provide evidence on negative campaigning *in practice* that complements political theory about how and why actors use negative campaigning.

They are interested in how real organisations use twitter, meaning their findings are not subject to the same external validity concerns generated by the unrepresentativeness of twitter.

## Tones from a Narrowing Race: Results

:::::: {.cols data-latex=""}

::: {.col data-latex="{0.5\textwidth}"}
The authors' analysis, which uses the Opinion Observer lexicon (I think available with the "bing" option in tidytext's `get_sentiments()`) finds a similar pattern of sentiment scores in the different campaigns over time
:::

::: {.col data-latex="{0.05\textwidth}"}
\ 
:::

::: {.col data-latex="{0.45\textwidth}"}

\begin{figure}
\includegraphics[width=\linewidth]{images/narrowing-fig3.png}
\end{figure}

:::

::::::

## Tones from a Narrowing Race: Results

:::::: {.cols data-latex=""}

::: {.col data-latex="{0.5\textwidth}"}
They find a significant relationship between the no campaign leading in the polls and the negativity of their tweets. The more NO leads, the more negatively they tweet. 

\medskip

In a simpler model, there is a significant relationship between the number of days to the election and the negativity of the YES campaign. That is, the decline in positivity as the election is statistically significant.

\medskip


There is also evidence that the YES campaign tweeted more negatively the day after each debate (which they were said to have lost)
:::

::: {.col data-latex="{0.05\textwidth}"}
\ 
:::

::: {.col data-latex="{0.45\textwidth}"}

\begin{figure}
\only<1>{\includegraphics[width=\linewidth]{images/narrowing-table1.png}}
\only<2>{\includegraphics[width=\linewidth]{images/narrowing-fig1.png}}
\end{figure}

:::

::::::

## Discusion

What does this tell us about the scottish election campagin?

What does this tell us about negative communication in political campaigns in general?

What potential confounding factors might explain the results?

## Other examples

- [Baylis and Obradovich et al.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0195750) explore how weather affects sentiment
- [Schwalbach](https://www.cambridge.org/core/journals/european-political-science-review/article/abs/going-in-circles-the-influence-of-the-electoral-cycle-on-the-party-behaviour-in-parliament/B4693B1A27049DB3BCE314F32D6BD1EB) looks at how political debate varies in sentiment through the legislative period, depending on their involvement in the government.
- [Wang et al.](https://www.nature.com/articles/s41562-022-01312-y) explore how the COVID-19 pandemic affected how people expressed sentiment
- [Arratia et al.](https://link.springer.com/chapter/10.1007/978-3-030-66891-4_9) describe how sentiment analysis is used to provide information about financial markets

# Wrapup and Outlook

## Wrapup

So far in this course, we have covered:

- How to get texts from tricky places
- How to process them and manipulate them
- How to represent them in different ways
- How to find the similarity between texts
- Hot to find what texts are about

Now we know how to find out what emotions are present in texts



## Outlook

We said sentiment analysis is a special case of classification. We will explore this in detail next week when we cover **supervised text classification**. 

We'll be training machine learning classifiers to assign documents to predefined classes, and learning how to evaluate how well these work.

This is the last major technique we will learn, before the penultimate session where we will find about how to apply a range of the techniques we have seen using the fanciest most up-to-date methods.

## Homework assignment (optional)

Take a protocol from the Bundestag and process it as you did in Assignment 1

Using a German sentiment lexicon, calculate a sentiment score for each speech.

Show the average sentiment score for each party.

Show the 5 speeches with the most positive sentiment, and the 5 speeches with the most negative sentiment.