{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation instructions\n",
    "\n",
    "[![colab badge](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mcallaghan/text-as-data/blob/master/Session-11-Spacy-and-Transformers/BERT.ipynb)\n",
    "\n",
    "To install the required libraries, you will need to do the following in a terminal shell (or prepend a ! to each line and run in colab)\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "pip install datasets\n",
    "pip install torch\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from torch import tensor\n",
    "from torch.nn import Sigmoid, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9998040795326233},\n",
       " {'label': 'POSITIVE', 'score': 0.999700665473938}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "pipe([\"This movie was really bad\", \"I loved watching this movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Employee_Engagement_Inclusion_And_Diversity',\n",
       "  'score': 0.9726636409759521}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"nbroad/ESG-BERT\")\n",
    "texts = [\n",
    "    \"The Hertie School is committed to embedding and mainstreaming diversity, equity and inclusion into all areas of its activities.\"\n",
    "]\n",
    "pipe(\"The Hertie School is committed to embedding and mainstreaming diversity, equity and inclusion into all areas of its activities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/max/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Large language models can be useful. However, language models are not',\n",
       " 'well validated and they are very easy to test.  When comparing',\n",
       " 'language models, a best-practice guideline is to use a large number of',\n",
       " 'different languages (in different ways), with']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import wrap\n",
    "run_galactica = False\n",
    "if run_galactica:\n",
    "    pipe = pipeline(\"text-generation\", model=\"facebook/galactica-1.3b\")\n",
    "else:\n",
    "    pipe = pipeline(\"text-generation\")\n",
    "    \n",
    "res = pipe(\"Large language models can be useful. However,\")\n",
    "wrap(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "run_galactica = False\n",
    "if run_galactica:\n",
    "    #pipe = pipeline(\"text-generation\", model=\"facebook/galactica-1.3b\")\n",
    "    #res = pipe(\"What are the benefits of taking the Text as Data course at Hertie\", max_length=200)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-1.3b\")\n",
    "    model = OPTForCausalLM.from_pretrained(\"facebook/galactica-1.3b\", device_map=\"auto\")\n",
    "\n",
    "    input_text = \"Wiki page about the Text as Data Course at the Hertie School of Governance. \\n#Introduction\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids, max_length=200)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.46396902203559875,\n",
       "  'token': 339,\n",
       "  'token_str': ' win',\n",
       "  'sequence': 'The GOP is going to win this election'},\n",
       " {'score': 0.46036452054977417,\n",
       "  'token': 2217,\n",
       "  'token_str': ' lose',\n",
       "  'sequence': 'The GOP is going to lose this election'},\n",
       " {'score': 0.011407189071178436,\n",
       "  'token': 8052,\n",
       "  'token_str': ' steal',\n",
       "  'sequence': 'The GOP is going to steal this election'},\n",
       " {'score': 0.008465033955872059,\n",
       "  'token': 11781,\n",
       "  'token_str': ' dominate',\n",
       "  'sequence': 'The GOP is going to dominate this election'},\n",
       " {'score': 0.006690724287182093,\n",
       "  'token': 9808,\n",
       "  'token_str': ' sweep',\n",
       "  'sequence': 'The GOP is going to sweep this election'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "unmasker(f\"The GOP is going to {unmasker.tokenizer.mask_token} this election\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take some texts and labels\n",
    "texts, y = zip(\n",
    "    *[\n",
    "        (\"Climate change is impacting human systems\", 1),\n",
    "        (\"Climate change is caused by fossil fuels\", 0),\n",
    "        (\"Agricultural yields are affected by climate change\", 1),\n",
    "        (\"System change not climate change\", 0),\n",
    "        (\"higher temperatures are impacting human health\", 1),\n",
    "        (\"Forest fires are becoming more frequent due to climate change\", 1),\n",
    "        (\"Machine learning can read texts\", 0),\n",
    "        (\"AI can help solve climate change!\", 0),\n",
    "        (\"We need to save gas this winter\", 0),\n",
    "        (\"More frequent droughts are impacting crop yields\", 1),\n",
    "        (\"Many communities are affected by rising sea levels\", 1),\n",
    "        (\"Global emissions continue to rise\", 0),\n",
    "        (\"Ecosystems are increasingly impacted by rising temperatures\", 1),\n",
    "        (\"Emissions from fossil fuels need to decline\", 0),\n",
    "        (\"Anthropogenic climate change is impacting vulnerable communities\", 1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470615d56f35470cb07a1b27118605c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 15\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# To use these with transformers, we are going to need to get them into the right format.\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# First we'll put them into a HuggingFace Dataset object\n",
    "dataset = Dataset.from_dict({\"text\": texts, \"label\": y})\n",
    "\n",
    "# And now we need to tokenize the texts, using the pretrained tokenizer from climatebert\n",
    "model_name = \"climatebert/distilroberta-base-climate-f\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=False)\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can wrap this into one function that turns any set of texts (and optional labels)\n",
    "# into a tokenized huggingface dataset\n",
    "def datasetify(x, tokenizer, y=None):\n",
    "    data_dict = {\"text\": x}\n",
    "    if y is not None:\n",
    "        data_dict[\"label\"] = y\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=False)\n",
    "\n",
    "    return dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ddda136f284ba68309433c4d538c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/max/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.6386734247207642, metrics={'train_runtime': 77.8341, 'train_samples_per_second': 0.578, 'train_steps_per_second': 0.077, 'total_flos': 11839997491200.0, 'train_loss': 0.6386734247207642, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to load our model, and instantiate a Trainer class\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# We set num_labels to 2 for binary classification, as we have two classes - positive and negative\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# The trainer class needs to be supplied with a model, and a dataset (and will also accept TrainingArguments and validation data)\n",
    "trainer = Trainer(model=model, train_dataset=datasetify(texts, tokenizer, y))\n",
    "# Once this has been instantiated we can apply the train() method\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c01a8d19ffe49ce9cce6958436cff9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.00496399,  0.32894096],\n",
       "       [ 0.27721342, -0.09202398],\n",
       "       [ 0.17405722, -0.02745686]], dtype=float32), label_ids=array([1, 0, 0]), metrics={'test_loss': 0.555717945098877, 'test_runtime': 1.6402, 'test_samples_per_second': 1.829, 'test_steps_per_second': 0.61})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To generate predictions, we just need to supply a dataset to the predict method\n",
    "new_texts = [\n",
    "    \"climate change is impacting terrestrial ecosystems\",\n",
    "    \"Machine Learning will solve climate change\",\n",
    "    \"Fossil fuels are responsible for rising temperature\",\n",
    "]\n",
    "\n",
    "\n",
    "pred = trainer.predict(datasetify(new_texts, tokenizer, [1, 0, 0]))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26354/3656768894.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  activation(tensor(pred.predictions))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4197, 0.5803],\n",
       "        [0.5913, 0.4087],\n",
       "        [0.5502, 0.4498]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, the model output gives us logits. If these are negative, then the prediction\n",
    "# is negative, if they are positive, the prediction is positive.\n",
    "# We can turn these into probabilities with an activation function\n",
    "from torch import tensor\n",
    "from torch.nn import Sigmoid, Softmax\n",
    "\n",
    "activation = (\n",
    "    Softmax()\n",
    ")  # Since we have two *exclusive classes*, we use the Softmax function\n",
    "activation(tensor(pred.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5012, 0.5815],\n",
       "        [0.5689, 0.4770],\n",
       "        [0.5434, 0.4931]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = (\n",
    "    Sigmoid()\n",
    ")  # With a Sigmoid function, the probabilities don't need to add up to 1 (useful for multilabel classification)\n",
    "activation(tensor(pred.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2c6fe8600a4417b3bb993bc5094e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c356598cca4022b43bca9d53b57883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26354/898640710.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return activation(tensor(logits)).numpy()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22592835, 0.77407163],\n",
       "       [0.78818536, 0.21181463],\n",
       "       [0.6920183 , 0.3079818 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to always get probabilities, we can subclass Trainer and add a new predict_proba method\n",
    "\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "\n",
    "class ProbTrainer(Trainer):\n",
    "    def predict_proba(self, test_dataset: Dataset) -> PredictionOutput:\n",
    "        logits = self.predict(test_dataset).predictions\n",
    "        if logits.shape[1] > 2:\n",
    "            activation = Sigmoid()\n",
    "        else:\n",
    "            activation = Softmax()\n",
    "        return activation(tensor(logits)).numpy()\n",
    "\n",
    "\n",
    "trainer = ProbTrainer(model=model, train_dataset=datasetify(texts, tokenizer, y))\n",
    "trainer.train()\n",
    "\n",
    "pred = trainer.predict_proba(datasetify(new_texts, tokenizer))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "  \"batch_size\": [16, 32],\n",
    "  \"learning_rate\": [5e-5, 3e-5, 2e-5],\n",
    "  \"number of epochs\": [2,3,4]\n",
    "}\n",
    "import itertools\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in itertools.product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "param_space = list(product_dict(**params))\n",
    "len(param_space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "for p in param_space:\n",
    "    break\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=p[\"number of epochs\"],\n",
    "        learning_rate=p[\"learning_rate\"],\n",
    "        per_device_train_batch_size=p[\"batch_size\"],\n",
    "        output_dir=\"out\"\n",
    "    )\n",
    "    trainer = ProbTrainer(model=model, train_dataset=datasetify(texts, tokenizer, y), args=training_args)\n",
    "    trainer.train()                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
